const axios = require('axios');
const fs = require('fs').promises;
const path = require('path');
const logger = require('./logger');

/**
 * Llama 3 integration for profile compatibility analysis
 */

/**
 * Default endpoint for Llama 3 API
 * You can modify this URL to point to your local or remote Llama 3 endpoint
 */
const DEFAULT_LLM_ENDPOINT = 'http://localhost:11434/api/generate';

// Alternative endpoints to try if the default fails
const ALTERNATIVE_ENDPOINTS = [
    'http://127.0.0.1:11434/api/generate',   // Some systems prefer 127.0.0.1 over localhost
    'http://localhost:8000/api/generate',    // Common alternative port
    'http://localhost:1234/v1/chat/completions'  // Compatible OpenAI-style endpoint
];

// Track the last working endpoint
let lastWorkingEndpoint = null;

/**
 * Get the Llama 3 endpoint URL from environment, user preferences, or fallback to default
 * @param {Object} userPreferences Optional user preferences that may contain LLM endpoint setting
 * @returns {string} The endpoint URL
 */
function getLlamaEndpoint(userPreferences = null) {
    // If we have a known working endpoint from previous checks, use that first
    if (lastWorkingEndpoint) {
        logger.log(`Using previously successful endpoint: ${lastWorkingEndpoint}`, logger.LOG_LEVELS.DEBUG, 'LLM');
        return lastWorkingEndpoint;
    }
    
    // First check environment variable
    if (process.env.LLAMA_ENDPOINT) {
        return process.env.LLAMA_ENDPOINT;
    }
    
    // Then check user preferences if available
    if (userPreferences && 
        userPreferences.llmSettings && 
        userPreferences.llmSettings.endpoint) {
        return userPreferences.llmSettings.endpoint;
    }
    
    // Finally default to the built-in endpoint
    return DEFAULT_LLM_ENDPOINT;
}

/**
 * Analyze a profile using Llama 3 API to determine compatibility
 * @param {string} prompt The prompt to send to Llama 3
 * @param {Object} userPreferences User preferences (including LLM settings)
 * @returns {Object} Analysis result with score and explanation
 */
async function analyzeWithLlama(prompt, userPreferences) {
    // Get endpoint, preferring lastWorkingEndpoint if available
    const endpoint = lastWorkingEndpoint || getLlamaEndpoint(userPreferences);
    // Increased timeout to prevent socket hang-up errors
    const options = { timeout: 30000 };
    
    try {
        logger.log(`Sending request to Llama API at ${endpoint}`, logger.LOG_LEVELS.DEBUG, 'LLM');
        logger.log(`Using prompt length: ${prompt.length} characters`, logger.LOG_LEVELS.DEBUG, 'LLM');
        
        // Attempt to send the request to the Llama API
        const response = await axios.post(endpoint, {
            model: 'llama3:latest',
            prompt: prompt,
            stream: false,
            temperature: 0.2,
            max_tokens: 150, // Reduced from 300 to help with faster responses
            stop: ["</answer>"]
        }, options);
        
        // Check if the response contains valid data
        if (!response.data) {
            logger.log('Empty response from Llama API', logger.LOG_LEVELS.WARNING, 'LLM');
            return { score: 0.5, explanation: 'Unable to analyze profile: Empty response from LLM API', fullResponse: null };
        }
        
        // Extract the generated text from the response
        const responseText = response.data.response || response.data.generated_text || '';
        
        if (!responseText) {
            logger.log('No generated text in response', logger.LOG_LEVELS.WARNING, 'LLM');
            return { score: 0.5, explanation: 'Unable to analyze profile: No text generated by LLM API', fullResponse: null };
        }
        
        logger.log('Llama API response received successfully', logger.LOG_LEVELS.SUCCESS, 'LLM');
        logger.log(`Response text (first 50 chars): ${responseText.substring(0, 50)}...`, logger.LOG_LEVELS.DEBUG, 'LLM');
        
        // Parse the response to extract compatibility score
        const scoreMatch = responseText.match(/compatibility score:\s*(0\.\d+|1\.0|1)/i) || 
                           responseText.match(/score:\s*(0\.\d+|1\.0|1)/i) ||
                           responseText.match(/(0\.\d+|1\.0|1)/);
                           
        // Get the explanation - everything after the score
        let explanation = responseText;
        if (scoreMatch && scoreMatch.index) {
            explanation = responseText.substring(scoreMatch.index + scoreMatch[0].length).trim();
        }
        
        // Clean up the explanation
        explanation = explanation.replace(/^[,\s.:;-]+/, '').trim();
        
        let score = 0.5; // Default to neutral
        
        if (scoreMatch && scoreMatch[1]) {
            score = parseFloat(scoreMatch[1]);
            logger.log(`Extracted compatibility score: ${score}`, logger.LOG_LEVELS.DEBUG, 'LLM');
        } else {
            logger.log('Could not extract a specific score from LLM response, using default 0.5', logger.LOG_LEVELS.WARNING, 'LLM');
        }
        
        // Safety check for the score to ensure it's within bounds
        if (isNaN(score) || score < 0 || score > 1) {
            logger.log(`Invalid score value: ${score}, defaulting to 0.5`, logger.LOG_LEVELS.WARNING, 'LLM');
            score = 0.5;
        }
        
        return {
            score,
            explanation: explanation || 'No explanation provided',
            fullResponse: responseText
        };
    } catch (error) {
        logger.log(`Error in LLM analysis: ${error.message}`, logger.LOG_LEVELS.ERROR, 'LLM');
        logger.log('Error details:', error.stack, logger.LOG_LEVELS.DEBUG, 'LLM');
        
        // Provide more specific error messages based on common errors
        let errorMsg = `LLM API error: ${error.message}`;
        
        if (error.code === 'ECONNREFUSED') {
            errorMsg = 'Connection refused. Please check if the Ollama server is running.';
        } else if (error.code === 'ETIMEDOUT') {
            errorMsg = 'Connection timed out. The LLM request took too long to process.';
        } else if (error.message.includes('socket hang up')) {
            errorMsg = 'Socket hang up. The connection was terminated during processing (possibly due to a large prompt).';
        }
        
        return {
            score: 0.5, // Default neutral score on error
            explanation: `Unable to analyze profile: ${errorMsg}`,
            error: error.message,
            fullResponse: null
        };
    }
}

/**
 * Generate a prompt for compatibility analysis
 * @param {Object} profileData Profile information
 * @param {Object} userPreferences User preferences
 * @returns {string} A formatted prompt for the LLM
 */
function generateCompatibilityPrompt(profileData, userPreferences) {
    // Extract user interests
    const userInterests = userPreferences.interests || [];
    const userAvoidKeywords = userPreferences.avoidKeywords || [];
    
    // Extract profile information
    const profileName = profileData.name || 'Unknown';
    const profileAge = profileData.age || 'Unknown';
    const profileBio = profileData.bio || '';
    const profileInterests = profileData.attributes || [];
    
    // Create a simpler, more structured prompt
    const prompt = `
You are a dating profile compatibility assistant.

User's preferences:
- Interests: ${userInterests.join(', ') || 'None specified'}
- Keywords to avoid: ${userAvoidKeywords.join(', ') || 'None specified'}

Profile to evaluate:
- Name: ${profileName}
- Age: ${profileAge}
- Bio: ${profileBio.substring(0, 200)}${profileBio.length > 200 ? '...' : ''}
- Interests: ${profileInterests.join(', ') || 'None specified'}

Compare these two profiles and provide:
1. A compatibility score from 0.0 to 1.0 (where 0.0 is completely incompatible and 1.0 is perfect match)
2. A brief explanation for your score

Format your response as:
Compatibility score: [number between 0.0-1.0]
[Your brief explanation]
`.trim();

    return prompt;
}

/**
 * Check if the Llama API is accessible
 * @param {Object} userPreferences User preferences for Llama settings
 * @param {number} timeout Optional timeout in milliseconds (default: 5000)
 * @returns {Promise<boolean>} True if the API is accessible
 */
async function checkLlamaApiConnection(userPreferences = null, timeout = 5000) {
    const primaryEndpoint = getLlamaEndpoint(userPreferences);
    
    // Handle different endpoint formats correctly
    let tagsEndpoint;
    if (primaryEndpoint.includes('/api/generate')) {
        tagsEndpoint = primaryEndpoint.replace('/api/generate', '/api/tags');
    } else if (primaryEndpoint.includes('/generate')) {
        tagsEndpoint = primaryEndpoint.replace('/generate', '/tags');
    } else if (primaryEndpoint.includes('/v1/chat/completions')) {
        tagsEndpoint = primaryEndpoint.replace('/v1/chat/completions', '/v1/models');
    } else {
        // Default fallback
        tagsEndpoint = primaryEndpoint + '/tags';
    }
    
    logger.log(`Checking Llama API at primary endpoint: ${primaryEndpoint}`, logger.LOG_LEVELS.DEBUG, 'LLM');
    logger.log(`Using tags endpoint: ${tagsEndpoint}`, logger.LOG_LEVELS.DEBUG, 'LLM');
    
    try {
        // Check if the API is accessible
        const response = await axios.get(tagsEndpoint, { timeout });
        
        if (response.data && response.data.models) {
            logger.log(`Available models: ${response.data.models.join(', ')}`, logger.LOG_LEVELS.DEBUG, 'LLM');
            // Store this as a working endpoint
            lastWorkingEndpoint = primaryEndpoint;
            return true;
        }
        
        logger.log('No models found in API response', logger.LOG_LEVELS.WARNING, 'LLM');
        return false;
    } catch (error) {
        logger.log(`Primary Llama API endpoint failed: ${error.message}`, logger.LOG_LEVELS.WARNING, 'LLM');
        
        // Try alternative endpoints if the primary fails
        for (const altEndpoint of ALTERNATIVE_ENDPOINTS) {
            logger.log(`Trying alternative Llama API endpoint: ${altEndpoint}`, logger.LOG_LEVELS.DEBUG, 'LLM');
            
            try {
                // Handle different endpoint formats correctly for alternatives too
                let altTagsEndpoint;
                if (altEndpoint.includes('/api/generate')) {
                    altTagsEndpoint = altEndpoint.replace('/api/generate', '/api/tags');
                } else if (altEndpoint.includes('/generate')) {
                    altTagsEndpoint = altEndpoint.replace('/generate', '/tags');
                } else if (altEndpoint.includes('/v1/chat/completions')) {
                    altTagsEndpoint = altEndpoint.replace('/v1/chat/completions', '/v1/models');
                } else {
                    // Default fallback
                    altTagsEndpoint = altEndpoint + '/tags';
                }
                
                const response = await axios.get(altTagsEndpoint, { timeout });
                
                if (response.data && response.data.models) {
                    logger.log(`Alternative Llama API endpoint successful: ${altEndpoint}`, logger.LOG_LEVELS.SUCCESS, 'LLM');
                    logger.log(`Updated LLM endpoint to working alternative: ${altEndpoint}`, logger.LOG_LEVELS.SUCCESS, 'LLM');
                    
                    // Store the working alternative endpoint
                    lastWorkingEndpoint = altEndpoint;
                    return true;
                }
            } catch (altError) {
                logger.log(`Alternative Llama API endpoint failed: ${altError.message}`, logger.LOG_LEVELS.DEBUG, 'LLM');
            }
        }
        
        return false;
    }
}

module.exports = {
    analyzeWithLlama,
    checkLlamaApiConnection,
    getLlamaEndpoint
}; 